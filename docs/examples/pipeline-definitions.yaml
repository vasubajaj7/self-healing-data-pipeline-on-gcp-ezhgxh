version: '1.0.0'
description: 'Example pipeline definitions for the self-healing data pipeline'
pipelines:
  - pipeline_id: 'pipe_gcs_customer_data'
    name: 'GCS Customer Data Ingestion'
    description: 'Ingests customer data from GCS bucket into BigQuery with data quality validation and self-healing'
    pipeline_type: 'GCS_INGESTION'
    source_id: 'sales_data_gcs'
    target_dataset: 'customer_data'
    target_table: 'customers'
    dag_id: 'gcs_customer_data_ingestion'
    is_active: true
    transformation_config:
      file_pattern: 'customers/*.csv'
      file_format: 'CSV'
      delimiter: ','
      header_rows: 1
      skip_leading_rows: 1
      encoding: 'UTF-8'
      transformations:
        - column: 'customer_id'
          type: 'string_transform'
          operation: 'trim'
        - column: 'email'
          type: 'string_transform'
          operation: 'lowercase'
        - column: 'registration_date'
          type: 'date_transform'
          operation: 'parse'
          format: '%Y-%m-%d'
    quality_config:
      enabled: true
      quality_threshold: 0.9
      rule_group: 'customer_data'
      additional_rules: ['rule-content-001', 'rule-content-003']
      validation_mode: 'STRICT'
      on_failure: 'SELF_HEAL'
    self_healing_config:
      enabled: true
      confidence_threshold: 0.85
      rule_set: 'data_quality'
      max_retry_attempts: 3
      approval_required: false
      notification_on_healing: true
    scheduling_config:
      schedule_interval: '0 2 * * *'
      start_date: '2023-06-01'
      end_date: null
      catchup: false
      depends_on_past: false
      wait_for_downstream: false
      max_active_runs: 1
    execution_config:
      timeout_seconds: 3600
      retries: 3
      retry_delay_seconds: 300
      email_on_failure: true
      email_on_retry: false
      priority: 'HIGH'
    performance_config:
      batch_size: 1000
      parallel_processing: true
      max_parallel_workers: 5
      bigquery_write_disposition: 'WRITE_TRUNCATE'
      bigquery_create_disposition: 'CREATE_IF_NEEDED'
    metadata:
      owner: 'data_engineering_team'
      tags: ['customer', 'gcs', 'daily']
      criticality: 'HIGH'
      data_classification: 'SENSITIVE'
      documentation_url: 'https://wiki.example.com/pipelines/customer_data'

  - pipeline_id: 'pipe_cloudsql_sales'
    name: 'Cloud SQL Sales Data Extraction'
    description: 'Extracts sales data from Cloud SQL database into BigQuery with incremental loading'
    pipeline_type: 'CLOUDSQL_EXTRACTION'
    source_id: 'customer_db'
    target_dataset: 'sales_metrics'
    target_table: 'sales'
    dag_id: 'cloudsql_sales_extraction'
    is_active: true
    transformation_config:
      extraction_query: "SELECT * FROM sales WHERE sale_date >= '{{ ds }}' AND sale_date < '{{ tomorrow_ds }}'"
      incremental_key: 'sale_date'
      incremental_type: 'DATE'
      transformations:
        - column: 'total_amount'
          type: 'numeric_transform'
          operation: 'round'
          precision: 2
        - column: 'discount_percent'
          type: 'numeric_transform'
          operation: 'multiply'
          factor: 100
    quality_config:
      enabled: true
      quality_threshold: 0.95
      rule_group: 'sales_metrics'
      additional_rules: ['rule-content-002', 'rule-content-004']
      validation_mode: 'STRICT'
      on_failure: 'SELF_HEAL'
    self_healing_config:
      enabled: true
      confidence_threshold: 0.9
      rule_set: 'data_quality'
      max_retry_attempts: 3
      approval_required: false
      notification_on_healing: true
    scheduling_config:
      schedule_interval: '0 3 * * *'
      start_date: '2023-06-01'
      end_date: null
      catchup: false
      depends_on_past: false
      wait_for_downstream: true
      max_active_runs: 1
    execution_config:
      timeout_seconds: 7200
      retries: 3
      retry_delay_seconds: 300
      email_on_failure: true
      email_on_retry: false
      priority: 'HIGH'
    performance_config:
      batch_size: 5000
      parallel_processing: true
      max_parallel_workers: 3
      bigquery_write_disposition: 'WRITE_APPEND'
      bigquery_create_disposition: 'CREATE_IF_NEEDED'
    metadata:
      owner: 'data_engineering_team'
      tags: ['sales', 'cloudsql', 'daily', 'incremental']
      criticality: 'HIGH'
      data_classification: 'SENSITIVE'
      documentation_url: 'https://wiki.example.com/pipelines/sales_data'

  - pipeline_id: 'pipe_api_product_catalog'
    name: 'Product Catalog API Ingestion'
    description: 'Ingests product catalog data from external API into BigQuery'
    pipeline_type: 'API_INGESTION'
    source_id: 'product_api'
    target_dataset: 'product_catalog'
    target_table: 'products'
    dag_id: 'api_product_catalog_ingestion'
    is_active: true
    transformation_config:
      endpoint_path: '/products'
      method: 'GET'
      request_params:
        limit: 100
        include_details: true
      response_mapping:
        root_path: 'data.products'
        mappings:
          product_id: 'id'
          product_name: 'name'
          description: 'description'
          category: 'category.name'
          price: 'price.amount'
          currency: 'price.currency'
          in_stock: 'inventory.in_stock'
          stock_quantity: 'inventory.quantity'
      transformations:
        - column: 'price'
          type: 'numeric_transform'
          operation: 'round'
          precision: 2
        - column: 'product_name'
          type: 'string_transform'
          operation: 'trim'
    quality_config:
      enabled: true
      quality_threshold: 0.9
      rule_group: 'product_catalog'
      additional_rules: ['rule-schema-003']
      validation_mode: 'LENIENT'
      on_failure: 'SELF_HEAL'
    self_healing_config:
      enabled: true
      confidence_threshold: 0.85
      rule_set: 'data_quality'
      max_retry_attempts: 3
      approval_required: false
      notification_on_healing: true
    scheduling_config:
      schedule_interval: '0 4 * * *'
      start_date: '2023-06-01'
      end_date: null
      catchup: false
      depends_on_past: false
      wait_for_downstream: false
      max_active_runs: 1
    execution_config:
      timeout_seconds: 3600
      retries: 5
      retry_delay_seconds: 300
      email_on_failure: true
      email_on_retry: false
      priority: 'MEDIUM'
    performance_config:
      batch_size: 100
      parallel_processing: false
      rate_limit: 60
      bigquery_write_disposition: 'WRITE_TRUNCATE'
      bigquery_create_disposition: 'CREATE_IF_NEEDED'
    metadata:
      owner: 'data_engineering_team'
      tags: ['product', 'api', 'daily']
      criticality: 'MEDIUM'
      data_classification: 'INTERNAL'
      documentation_url: 'https://wiki.example.com/pipelines/product_catalog'

  - pipeline_id: 'pipe_daily_sales_metrics'
    name: 'Daily Sales Metrics Calculation'
    description: 'Calculates daily sales metrics and aggregations for reporting'
    pipeline_type: 'DATA_PROCESSING'
    source_id: 'analytics_bq'
    target_dataset: 'sales_metrics'
    target_table: 'daily_sales'
    dag_id: 'daily_sales_metrics_processing'
    is_active: true
    transformation_config:
      sql_query: "SELECT sale_date, COUNT(*) as transaction_count, SUM(quantity) as total_quantity, SUM(total_amount) as total_sales, AVG(total_amount) as avg_sale_value, SUM(CASE WHEN discount_percent > 0 THEN 1 ELSE 0 END) as discounted_sales_count FROM `sales_metrics.sales` WHERE sale_date = '{{ ds }}' GROUP BY sale_date"
      transformations:
        - column: 'total_sales'
          type: 'numeric_transform'
          operation: 'round'
          precision: 2
        - column: 'avg_sale_value'
          type: 'numeric_transform'
          operation: 'round'
          precision: 2
    quality_config:
      enabled: true
      quality_threshold: 0.95
      rule_group: 'sales_metrics'
      additional_rules: ['rule-statistical-004']
      validation_mode: 'STRICT'
      on_failure: 'SELF_HEAL'
    self_healing_config:
      enabled: true
      confidence_threshold: 0.9
      rule_set: 'data_quality'
      max_retry_attempts: 3
      approval_required: false
      notification_on_healing: true
    scheduling_config:
      schedule_interval: '0 5 * * *'
      start_date: '2023-06-01'
      end_date: null
      catchup: false
      depends_on_past: false
      wait_for_downstream: true
      max_active_runs: 1
    execution_config:
      timeout_seconds: 1800
      retries: 3
      retry_delay_seconds: 300
      email_on_failure: true
      email_on_retry: false
      priority: 'HIGH'
    performance_config:
      bigquery_write_disposition: 'WRITE_APPEND'
      bigquery_create_disposition: 'CREATE_IF_NEEDED'
      use_legacy_sql: false
      query_timeout_seconds: 600
    metadata:
      owner: 'data_engineering_team'
      tags: ['sales', 'metrics', 'daily', 'aggregation']
      criticality: 'HIGH'
      data_classification: 'INTERNAL'
      documentation_url: 'https://wiki.example.com/pipelines/daily_sales_metrics'

pipeline_dependencies:
  - pipeline_id: 'pipe_daily_sales_metrics'
    depends_on: ['pipe_cloudsql_sales']
  - pipeline_id: 'pipe_inventory_sync'
    depends_on: ['pipe_api_product_catalog']

documentation:
  pipeline_types:
    GCS_INGESTION: 'Ingests data from Google Cloud Storage into BigQuery'
    CLOUDSQL_EXTRACTION: 'Extracts data from Cloud SQL databases into BigQuery'
    API_INGESTION: 'Ingests data from external APIs into BigQuery'
    DATA_PROCESSING: 'Processes data already in BigQuery using SQL transformations'
  
  configuration_sections:
    transformation_config: 'Defines how data is transformed during ingestion'
    quality_config: 'Configures data quality validation rules and thresholds'
    self_healing_config: 'Configures self-healing behavior for pipeline issues'
    scheduling_config: 'Defines when and how the pipeline is scheduled'
    execution_config: 'Configures execution parameters like timeouts and retries'
    performance_config: 'Optimizes performance aspects like batch size and parallelism'
  
  validation_modes:
    STRICT: "Fails the pipeline if quality validation doesn't meet threshold"
    LENIENT: 'Continues pipeline execution with warnings if validation fails'
  
  on_failure_actions:
    SELF_HEAL: 'Attempt to automatically fix issues using the self-healing engine'
    ALERT: "Generate alerts but don't attempt automatic fixes"
    FAIL: 'Fail the pipeline without attempting fixes'

examples:
  adding_new_pipeline: 'To add a new pipeline, create a new entry in the pipelines array with a unique pipeline_id and configure all required sections.'
  defining_dependencies: 'Use the pipeline_dependencies section to define execution dependencies between pipelines.'
  quality_rule_reference: 'Reference quality rules defined in quality-rules.yaml using rule_group or additional_rules properties.'

metadata:
  version: '1.0.0'
  last_updated: '2023-06-15'
  author: 'Data Engineering Team'
  contact: 'data-engineering@example.com'